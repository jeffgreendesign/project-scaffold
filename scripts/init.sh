#!/bin/bash
# ============================================================================
# init.sh — Single-file project scaffold distribution
# ============================================================================
# Generated by generate-init.sh — DO NOT EDIT DIRECTLY.
# Regenerate with: ./scripts/generate-init.sh
#
# This script creates all scaffold files in the current directory (or a
# specified target directory). It's the "compiled" version of scaffold/ —
# a single script you can curl | bash or copy-paste without cloning the repo.
#
# Usage:
#   ./scripts/init.sh [target-directory]
#   curl -sL <url>/init.sh | bash
# ============================================================================
set -euo pipefail

TARGET_DIR="${1:-.}"
mkdir -p "$TARGET_DIR"
TARGET_DIR="$(cd "$TARGET_DIR" && pwd)"

echo "Initializing project scaffold in $TARGET_DIR..."
echo ""

# Detect package manager
detect_package_manager() {
  if [ -f "$TARGET_DIR/pnpm-lock.yaml" ]; then
    echo "pnpm"
  elif [ -f "$TARGET_DIR/yarn.lock" ]; then
    echo "yarn"
  elif [ -f "$TARGET_DIR/package-lock.json" ]; then
    echo "npm"
  else
    echo "npm"
  fi
}

PKG_MANAGER=$(detect_package_manager)
echo "Detected package manager: $PKG_MANAGER"

# Helper: write a file, prompting before overwriting
write_file() {
  local filepath="$TARGET_DIR/$1"
  local dirpath
  dirpath="$(dirname "$filepath")"
  mkdir -p "$dirpath"

  if [ -f "$filepath" ]; then
    echo -n "  File exists: $1 — overwrite? [y/N] "
    read -r answer
    if [ "$answer" != "y" ] && [ "$answer" != "Y" ]; then
      echo "  Skipped: $1"
      return
    fi
  fi

  cat > "$filepath"
  echo "  Created: $1"
}

# Helper: write a file and make it executable
write_executable() {
  write_file "$1"
  chmod +x "$TARGET_DIR/$1"
}

write_file '.claude/commands/new-component.md' << 'SCAFFOLD_EOF__CLAUDE_COMMANDS_NEW-COMPONENT_MD'
Create a new component following project conventions:

## Instructions

1. Read CLAUDE.md for directory structure and naming conventions
2. Check existing components in `src/components/` for patterns

## Output

1. Create component file at `src/components/[name].tsx`
2. Create test file at `src/components/__tests__/[name].test.tsx`
3. Add smoke test to `src/components/__tests__/smoke.test.tsx`

## After Creating

1. Run `npm run gates`

SCAFFOLD_EOF__CLAUDE_COMMANDS_NEW-COMPONENT_MD

write_file '.claude/commands/validate.md' << 'SCAFFOLD_EOF__CLAUDE_COMMANDS_VALIDATE_MD'
Run all quality gates on the codebase:

1. Run `npm run gates`
2. Report results for each gate (typecheck, lint, test, build)
3. If any gate fails, identify the specific errors
4. Do NOT proceed with committing until all gates pass

SCAFFOLD_EOF__CLAUDE_COMMANDS_VALIDATE_MD

write_executable '.claude/hooks/session-start.sh' << 'SCAFFOLD_EOF__CLAUDE_HOOKS_SESSION-START_SH'
#!/bin/bash
set -euo pipefail

PROJECT_DIR="${CLAUDE_PROJECT_DIR:-$(pwd)}"
cd "$PROJECT_DIR"

echo "--- Session start: installing dependencies ---"

# Detect package manager
if [ -f "pnpm-lock.yaml" ]; then
  pnpm install --frozen-lockfile 2>&1
elif [ -f "yarn.lock" ]; then
  yarn install --frozen-lockfile 2>&1
elif [ -f "package-lock.json" ]; then
  npm ci 2>&1
elif [ -f "pyproject.toml" ]; then
  pip install -e ".[dev]" --quiet 2>&1
elif [ -f "requirements.txt" ]; then
  pip install -r requirements.txt --quiet 2>&1
fi

echo "--- Session start complete ---"

SCAFFOLD_EOF__CLAUDE_HOOKS_SESSION-START_SH

write_file '.claude/settings.json' << 'SCAFFOLD_EOF__CLAUDE_SETTINGS_JSON'
{
  "permissions": {
    "allow": [
      "Bash(npm run gates*)",
      "Bash(npm run lint*)",
      "Bash(npm run typecheck*)",
      "Bash(npm run test*)",
      "Bash(npm run build*)",
      "Bash(npm run dev*)",
      "Bash(git status*)",
      "Bash(git diff*)",
      "Bash(git log*)",
      "Bash(git add *)",
      "Bash(git commit *)"
    ],
    "deny": [
      "Bash(rm -rf *)",
      "Bash(git push --force*)",
      "Bash(git reset --hard*)",
      "Bash(DROP TABLE*)",
      "Bash(DROP DATABASE*)",
      "Bash(curl *)"
    ]
  }
}

SCAFFOLD_EOF__CLAUDE_SETTINGS_JSON

write_file '.cursor/rules/typescript.mdc' << 'SCAFFOLD_EOF__CURSOR_RULES_TYPESCRIPT_MDC'
---
description: TypeScript conventions and import patterns
globs: ["src/**/*.ts", "src/**/*.tsx"]
---

# TypeScript Rules

## Critical Rules

- ESM imports only — use `.js` extensions even for .ts files
- Never use `any` — use `unknown` and narrow with type guards
- Never use non-null assertions (`!`) — handle the null case

## Import Patterns

- Use `@/` path aliases for project imports
- External deps first, then internal, then relative
- No barrel exports (`index.ts` re-exports) — import directly

## Common Mistakes

- Forgetting `"use client"` when using React hooks
- Using `console.log()` instead of the project logger
- String interpolation in SQL queries (use parameterized queries)

SCAFFOLD_EOF__CURSOR_RULES_TYPESCRIPT_MDC

write_file '.env.example' << 'SCAFFOLD_EOF__ENV_EXAMPLE'
# =============================================================================
# Environment Variables
# =============================================================================
# Copy this file to .env and fill in the values.
# NEVER commit .env to version control.
#
# CUSTOMIZE: Add all environment variables your project uses.
# Mark each as REQUIRED or OPTIONAL.
# Include valid value examples or ranges.
# =============================================================================

# Server (REQUIRED)
PORT=3000
NODE_ENV=development

# Database (REQUIRED)
DATABASE_URL=postgresql://user:pass@localhost:5432/mydb

# Authentication (REQUIRED for production)
# API_KEY=your-api-key-here

# Feature Flags (OPTIONAL)
# ENABLE_DEBUG=false

SCAFFOLD_EOF__ENV_EXAMPLE

write_file '.github/workflows/changelog-check.yml' << 'SCAFFOLD_EOF__GITHUB_WORKFLOWS_CHANGELOG-CHECK_YML'
# Changelog Enforcement
# ============================================================================
# Ensures every PR that changes code also updates CHANGELOG.md.
# Prevents changelog drift — the #1 source of "what changed in this release?"
#
# Escape hatches:
# - Add the `skip-changelog` label for refactors, CI changes, etc.
# - Docs-only PRs (only .md files changed) are automatically skipped.
# ============================================================================

name: Changelog Check

on:
  pull_request:
    branches: [main]

jobs:
  changelog:
    # Skip if PR has the skip-changelog label
    if: "!contains(github.event.pull_request.labels.*.name, 'skip-changelog')"
    runs-on: ubuntu-latest
    timeout-minutes: 5

    steps:
      - uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Check for changelog update
        run: |
          # Get list of changed files
          CHANGED_FILES=$(git diff --name-only origin/main...HEAD)

          # Check if only markdown files changed (docs-only PR)
          NON_MD_FILES=$(echo "$CHANGED_FILES" | grep -v '\.md$' || true)

          if [ -z "$NON_MD_FILES" ]; then
            echo "✓ Docs-only PR — changelog update not required."
            exit 0
          fi

          # Code files changed — CHANGELOG.md must be updated
          if echo "$CHANGED_FILES" | grep -q '^CHANGELOG.md$'; then
            echo "✓ CHANGELOG.md was updated."
            exit 0
          fi

          echo "✗ CHANGELOG.md was not updated."
          echo ""
          echo "This PR changes code files but doesn't update CHANGELOG.md."
          echo ""
          echo "What to do:"
          echo "  1. Add an entry under '## [Unreleased]' in CHANGELOG.md"
          echo "  2. Use the format: '- Description of what changed'"
          echo "  3. Group under: Added, Changed, Fixed, Removed"
          echo ""
          echo "If this change doesn't need a changelog entry (refactor, CI, etc.):"
          echo "  Add the 'skip-changelog' label to this PR."
          exit 1

SCAFFOLD_EOF__GITHUB_WORKFLOWS_CHANGELOG-CHECK_YML

write_file '.github/workflows/ci.yml' << 'SCAFFOLD_EOF__GITHUB_WORKFLOWS_CI_YML'
# CI Pipeline
# ============================================================================
# Runs quality gates on every push to main and on pull requests targeting main.
#
# Design choices:
# - Uses dorny/paths-filter to skip CI on docs-only changes (saves minutes)
# - Reads Node version from .node-version (single source of truth for runtime)
# - Final step runs `npm run gates` — the same command developers run locally
#   and in pre-commit hooks. One command, everywhere, no drift.
# - 10-minute timeout prevents hung processes from burning Actions minutes
# ============================================================================

name: CI

on:
  push:
    branches: [main]
  pull_request:
    branches: [main]

# Cancel in-progress runs for the same branch/PR
concurrency:
  group: ${{ github.workflow }}-${{ github.ref }}
  cancel-in-progress: true

jobs:
  # Determine what changed to skip unnecessary work
  changes:
    runs-on: ubuntu-latest
    outputs:
      code: ${{ steps.filter.outputs.code }}
    steps:
      - uses: actions/checkout@v4
      - uses: dorny/paths-filter@v3
        id: filter
        with:
          filters: |
            code:
              - '**/*.ts'
              - '**/*.tsx'
              - '**/*.js'
              - '**/*.jsx'
              - '**/*.json'
              - '**/*.yaml'
              - '**/*.yml'
              - 'package.json'
              - 'pnpm-lock.yaml'
              - 'package-lock.json'

  quality:
    needs: changes
    # Skip quality checks if only docs changed
    if: needs.changes.outputs.code == 'true'
    runs-on: ubuntu-latest
    timeout-minutes: 10

    steps:
      - uses: actions/checkout@v4

      # Read Node version from .node-version — enforces the compatibility matrix.
      # If you hardcode the version here, it WILL drift from .node-version.
      - name: Read Node version
        id: node-version
        run: echo "version=$(cat .node-version)" >> "$GITHUB_OUTPUT"

      - uses: actions/setup-node@v4
        with:
          node-version: ${{ steps.node-version.outputs.version }}

      # Detect and use the correct package manager
      - name: Detect package manager
        id: pkg-manager
        run: |
          if [ -f "pnpm-lock.yaml" ]; then
            echo "manager=pnpm" >> "$GITHUB_OUTPUT"
            echo "install=pnpm install --frozen-lockfile" >> "$GITHUB_OUTPUT"
          elif [ -f "yarn.lock" ]; then
            echo "manager=yarn" >> "$GITHUB_OUTPUT"
            echo "install=yarn install --frozen-lockfile" >> "$GITHUB_OUTPUT"
          else
            echo "manager=npm" >> "$GITHUB_OUTPUT"
            echo "install=npm ci" >> "$GITHUB_OUTPUT"
          fi

      # Enable pnpm if needed (must happen before cache setup)
      - name: Enable pnpm
        if: steps.pkg-manager.outputs.manager == 'pnpm'
        run: corepack enable pnpm

      # Cache dependencies to speed up subsequent runs
      - uses: actions/setup-node@v4
        with:
          node-version: ${{ steps.node-version.outputs.version }}
          cache: ${{ steps.pkg-manager.outputs.manager }}

      - name: Install dependencies
        run: ${{ steps.pkg-manager.outputs.install }}

      # Run the single quality gate command.
      # This is the same command as pre-commit hooks and local dev.
      # CI should never run different checks than local — one command everywhere.
      - name: Run quality gates
        run: npm run gates

SCAFFOLD_EOF__GITHUB_WORKFLOWS_CI_YML

write_executable '.husky/pre-commit' << 'SCAFFOLD_EOF__HUSKY_PRE-COMMIT'
#!/bin/sh

# Quality gates — must pass before commit
# For full test suite, rely on CI. Pre-commit runs fast checks only.
npm run lint
npm run typecheck

SCAFFOLD_EOF__HUSKY_PRE-COMMIT

write_file 'AGENTS.md' << 'SCAFFOLD_EOF_AGENTS_MD_TEMPLATE'
# AGENTS.md

<!-- CUSTOMIZE: Only create this file if your project exposes an API, SDK, or tool interface. -->
<!-- This tells external AI agents how to USE your project (vs. CLAUDE.md which tells agents how to DEVELOP it). -->

## Quick Reference

| Aspect  | Value                          |
|---------|--------------------------------|
| API     | [REST / GraphQL / MCP], base URL [/api/v1] |
| Auth    | [Bearer token / API key / none] |
| Limits  | [Rate limits]                  |

## Endpoint Selection Guide

| User Intent          | Endpoint        | Key Params          |
|----------------------|-----------------|---------------------|
| [Intent 1]           | [METHOD /path]  | [params]            |
| [Intent 2]           | [METHOD /path]  | [params]            |

## Common Workflows

<!-- Show 2-3 typical multi-step workflows an agent would perform -->

### [Workflow Name]

1. [Step 1]
2. [Step 2]
3. [Step 3]

## Error Handling

| Error | Cause | Fix |
|-------|-------|-----|
| 401   | [cause] | [fix] |
| 429   | [cause] | [fix] |

SCAFFOLD_EOF_AGENTS_MD_TEMPLATE

write_file 'CHANGELOG.md' << 'SCAFFOLD_EOF_CHANGELOG_MD'
# Changelog

All notable changes to this project will be documented in this file.

Format: [Keep a Changelog](https://keepachangelog.com/)

## [Unreleased]

### Added

- Initial project setup

SCAFFOLD_EOF_CHANGELOG_MD

write_file 'CLAUDE.md' << 'SCAFFOLD_EOF_CLAUDE_MD_TEMPLATE'
# CLAUDE.md

<!-- CUSTOMIZE: Replace everything in [brackets] with your project values -->

## Project Overview

[One paragraph: what this project does, who it's for, what problem it solves.]

## Quick Reference

| Aspect          | Value                          |
|-----------------|--------------------------------|
| Package manager | [pnpm / npm / yarn]            |
| Runtime         | [Node.js >= 22 / Python >= 3.12] |
| Language        | [TypeScript (ESM) / Python]    |
| Framework       | [Next.js 16 / FastAPI / etc.]  |
| Lint            | `[pnpm lint]`                  |
| Test            | `[pnpm test]`                  |
| Type check      | `[pnpm typecheck]`            |
| Build           | `[pnpm build]`                 |
| **All checks**  | **`[pnpm gates]`**             |
| Node targets    | [>= 22 / >= 20 / >= 18]       |
| TS target       | [ESNext / ES2022 / etc.]       |

<!-- CUSTOMIZE: The compatibility matrix (Node targets, TS target) matters. -->
<!-- Agents will "modernize" syntax and silently drop support for older runtimes -->
<!-- if you don't explicitly state what you support. -->

## Development Commands

<!-- CUSTOMIZE: Every command the LLM might need, copy-paste ready -->
<!-- Group by purpose: dev, quality, testing, build -->

```bash
# Development
[pnpm dev]

# Quality gates (run before every commit)
[pnpm gates]

# Individual checks
[pnpm lint]
[pnpm typecheck]
[pnpm test]
[pnpm build]
```

## Architecture

### Key Systems

<!-- CUSTOMIZE: Each major subsystem gets a paragraph -->

<!-- Include: what it does, where it lives, how pieces connect -->

### Directory Map

<!-- CUSTOMIZE: Run `tree -L 2 src/` and annotate -->

```text
src/
├── [folder]/     # [purpose]
├── [folder]/     # [purpose]
└── [folder]/     # [purpose]
```

### Starting Points

<!-- CUSTOMIZE: List the 3-5 files/folders where most tasks begin. -->

<!-- This prevents the agent from exploring the wrong part of the repo. -->

<!-- Critical for repos with multiple top-level directories (app/, website/, docs/, etc.) -->

Most tasks start in `src/`. Key entry points:

|Task                |Start Here      |Why                      |
|--------------------|----------------|-------------------------|
|[Most common task]  |`[path/to/file]`|[What this file controls]|
|[Second most common]|`[path/to/file]`|[What this file controls]|
|[Third most common] |`[path/to/file]`|[What this file controls]|

## Code Conventions

<!-- CUSTOMIZE: The rules LLMs violate most often in YOUR project -->

<!-- Use the Rule-Bug-Prevention format for each: -->

### [Convention Category]

**Rule:** [What to do]
**Bug it prevents:** [What goes wrong without this rule]

```[lang]
# WRONG
[bad pattern]

# CORRECT
[good pattern]
```

## Common Mistakes

<!-- CUSTOMIZE: Group by severity -->

### Build Breakers

These will fail the build:

- [Mistake 1]
- [Mistake 2]

### Silent Bugs

These won't fail the build but cause problems:

- [Mistake 1]
- [Mistake 2]

## How to Add New [Things]

<!-- CUSTOMIZE: Step-by-step recipes for every common creation task -->

<!-- "Create file at X, add Y, run Z." No ambiguity. -->

### New [Component/Route/Endpoint]

1. Create `[path/to/new-thing]`
2. Add required fields: [list]
3. Follow this pattern:

   ```[lang]
   [minimal template]
   ```

4. Run `[pnpm gates]`

## Architecture Decisions

<!-- CUSTOMIZE: Why each major tech choice was made -->

<!-- "Do not suggest alternatives unless explicitly asked." -->

These choices are intentional. Do not suggest alternatives unless explicitly asked.

- **[Tool] over [Alternative]**: [Reason]
- **[Tool] over [Alternative]**: [Reason]

## Data Integrity Rules

<!-- CUSTOMIZE: Remove this section if your project doesn't handle data ingestion/sync. -->

<!-- For projects that read, transform, or persist data, these rules prevent silent data loss. -->

- **Never cap results with array slicing** (no `[:20]` or `.slice(0, 100)`) unless it's a deliberate, documented CLI flag. If 50 items were collected, all 50 must be saved.
- **Per-item error handling in loops is mandatory.** A single outer try/catch means one bad item kills processing for everything after it.
- **Log counts and error counts** for every ingestion/sync operation. "Synced 47/50 items, 3 failures" not silence.
- **Ordering and pagination must be deterministic.** Sort by a unique column. If sorting by a non-unique column, add a tiebreaker.
- **Never pass empty strings to database timestamp columns.** Convert to `null`/`None`.

## Public API Stability

<!-- CUSTOMIZE: Only include this section for libraries with external consumers. -->

<!-- Remove for apps and internal tools. -->

### Stable Exports (do not change without migration entry)

<!-- CUSTOMIZE: List your public API surface -->

- `[exportName]` — [what it does]
- `[exportName]` — [what it does]

**Rule:** Any change to a stable export requires:

1. A new entry in MIGRATION.md explaining the change
2. A semver-appropriate version bump
3. Deprecation notice on the old API if not a major version

## Debug Playbook

<!-- CUSTOMIZE: The 3-5 most common failure modes and their fixes. -->

<!-- Agents get stuck in retry loops when they hit failures they don't understand. -->

<!-- This section breaks those loops. -->

### If tests fail in CI but pass locally

- [Most likely cause, e.g., "Missing env var in CI — check .github/workflows/ci.yml"]
- [Second cause]

### If the build fails

- [Most likely cause, e.g., "TypeScript strict mode catches something the IDE missed — run `pnpm typecheck` locally"]
- [Second cause]

### If lint fails

- [Most likely cause, e.g., "Auto-fixable — run `pnpm lint --fix`"]

### If types fail

- [Most likely cause, e.g., "Missing type for new dependency — check if @types/[pkg] exists"]

## Environment Variables

<!-- CUSTOMIZE: Every env var with description -->

<!-- Mark REQUIRED vs OPTIONAL -->

|Variable|Required|Purpose       |
|--------|--------|--------------|
|`[VAR]` |Yes     |[What it does]|

## Documentation Sync Rules

<!-- CUSTOMIZE: List files that must be updated together -->

<!-- This is the #1 source of documentation drift -->

When updating [X], also update:

- [File A]
- [File B]

SCAFFOLD_EOF_CLAUDE_MD_TEMPLATE

write_file 'NOW.md' << 'SCAFFOLD_EOF_NOW_MD_TEMPLATE'
# What's Happening Now

<!-- This file prevents context loss between sessions. -->
<!-- Update it at the end of every work session. -->
<!-- Keep it under 100 lines. If it's longer, you're not updating often enough. -->

**Last Updated:** [YYYY-MM-DD]
**Project Status:** [Brief status — e.g., "MVP in progress", "95% complete"]

## Current Sprint

- [ ] [Active task 1]
- [ ] [Active task 2]

## Recently Completed

- [x] [Completed item] ([date])

## Blocked / Waiting

- [Item] — blocked on [reason]

## Next Actions

**High Priority:**

- [Task A]

**Low Priority:**

- [Task B]

SCAFFOLD_EOF_NOW_MD_TEMPLATE

write_file 'config/.node-version' << 'SCAFFOLD_EOF_CONFIG__NODE-VERSION'
22

SCAFFOLD_EOF_CONFIG__NODE-VERSION

write_file 'config/biome.json' << 'SCAFFOLD_EOF_CONFIG_BIOME_JSON'
{
  "$schema": "https://biomejs.dev/schemas/1.9.4/schema.json",
  "organizeImports": {
    "enabled": true
  },
  "formatter": {
    "enabled": true,
    "indentStyle": "tab",
    "indentWidth": 2,
    "lineWidth": 100
  },
  "javascript": {
    "formatter": {
      "quoteStyle": "single",
      "trailingCommas": "all",
      "semicolons": "always",
      "arrowParentheses": "always"
    }
  },
  "linter": {
    "enabled": true,
    "rules": {
      "recommended": true,
      "complexity": {
        "noExcessiveCognitiveComplexity": "warn",
        "noForEach": "warn",
        "useLiteralKeys": "error"
      },
      "correctness": {
        "noUnusedVariables": "error",
        "noUnusedImports": "error",
        "useExhaustiveDependencies": "warn"
      },
      "performance": {
        "noAccumulatingSpread": "warn",
        "noDelete": "warn"
      },
      "security": {
        "noDangerouslySetInnerHtml": "error"
      },
      "style": {
        "noNonNullAssertion": "warn",
        "useConst": "error",
        "useTemplate": "error",
        "noParameterAssign": "error"
      },
      "suspicious": {
        "noExplicitAny": "error",
        "noConsole": "warn"
      }
    }
  },
  "files": {
    "ignore": [
      "node_modules",
      "dist",
      "build",
      ".next",
      "coverage",
      "*.config.js",
      "*.config.ts"
    ]
  }
}

SCAFFOLD_EOF_CONFIG_BIOME_JSON

write_file 'config/tsconfig.json' << 'SCAFFOLD_EOF_CONFIG_TSCONFIG_JSON'
{
  "compilerOptions": {
    "strict": true,
    "noUncheckedIndexedAccess": true,
    "noUnusedLocals": true,
    "noUnusedParameters": true,
    "exactOptionalPropertyTypes": false,
    "moduleResolution": "bundler",
    "module": "ESNext",
    "target": "ESNext",
    "esModuleInterop": true,
    "skipLibCheck": true,
    "forceConsistentCasingInFileNames": true,
    "resolveJsonModule": true,
    "isolatedModules": true
  }
}

SCAFFOLD_EOF_CONFIG_TSCONFIG_JSON

write_file 'config/tsconfig.python-equiv.md' << 'SCAFFOLD_EOF_CONFIG_TSCONFIG_PYTHON-EQUIV_MD'
# Python Equivalents for TypeScript Config

<!-- This file maps the TypeScript tooling choices in this scaffold to their
     Python equivalents. Use it when setting up a Python project with the same
     level of strictness and quality gates. -->

## Linter + Formatter: Biome → Ruff

TypeScript uses Biome for linting and formatting. The Python equivalent is [Ruff](https://docs.astral.sh/ruff/).

```toml
# pyproject.toml
[tool.ruff]
line-length = 100
target-version = "py312"

[tool.ruff.lint]
select = [
    "E",    # pycodestyle errors
    "W",    # pycodestyle warnings
    "F",    # pyflakes
    "I",    # isort
    "N",    # pep8-naming
    "UP",   # pyupgrade
    "B",    # flake8-bugbear
    "A",    # flake8-builtins
    "S",    # flake8-bandit (security)
    "T20",  # flake8-print (catches print statements)
    "SIM",  # flake8-simplify
    "TCH",  # flake8-type-checking
    "ARG",  # flake8-unused-arguments
    "RUF",  # ruff-specific rules
]

[tool.ruff.format]
quote-style = "single"
indent-style = "tab"
```

## Type Checker: TypeScript strict → mypy / pyright

TypeScript's `strict: true` maps to strict mypy or pyright settings.

```toml
# pyproject.toml — mypy
[tool.mypy]
python_version = "3.12"
strict = true
warn_return_any = true
warn_unused_configs = true
disallow_untyped_defs = true
disallow_any_generics = true
no_implicit_optional = true
check_untyped_defs = true
```

```toml
# pyproject.toml — pyright (alternative)
[tool.pyright]
pythonVersion = "3.12"
typeCheckingMode = "strict"
```

## Test Runner: Vitest → pytest

```toml
# pyproject.toml
[tool.pytest.ini_options]
testpaths = ["tests"]
python_files = ["test_*.py"]
python_functions = ["test_*"]
addopts = "-v --tb=short"
```

## Quality Gates: `npm run gates` → `python -m gates`

Create a `gates` module or script that runs all checks:

```python
# gates.py (or as a package.json-style script in pyproject.toml)
"""
Quality gates — run before every commit.
Usage: python -m gates
"""
import subprocess
import sys

gates = [
    ("lint", ["ruff", "check", "."]),
    ("format", ["ruff", "format", "--check", "."]),
    ("typecheck", ["mypy", "."]),
    ("test", ["pytest"]),
]

failed = []
for name, cmd in gates:
    print(f"\n--- {name} ---")
    result = subprocess.run(cmd)
    if result.returncode != 0:
        failed.append(name)

if failed:
    print(f"\n✗ Failed gates: {', '.join(failed)}")
    sys.exit(1)
print("\n✓ All gates passed.")
```

Or in pyproject.toml with a task runner:

```toml
# pyproject.toml (using hatch or pdm scripts)
[tool.hatch.envs.default.scripts]
gates = [
    "ruff check .",
    "ruff format --check .",
    "mypy .",
    "pytest",
]
```

## Runtime Pin: `.node-version` → `.python-version`

```text
3.12
```

Use with pyenv or mise to ensure consistent Python versions.

## Package Manager: pnpm → uv / pip

- **uv** is the modern equivalent (fast, lockfile-based)
- **pip + pip-tools** is the traditional approach
- **poetry** / **pdm** are alternatives with built-in virtual env management

## Key Differences

| TypeScript                  | Python Equivalent           |
|-----------------------------|-----------------------------|
| `biome.json`                | `pyproject.toml` [tool.ruff] |
| `tsconfig.json` (strict)    | `pyproject.toml` [tool.mypy] |
| `vitest`                    | `pytest`                    |
| `.node-version`             | `.python-version`           |
| `pnpm install`              | `uv sync` / `pip install`   |
| `npm run gates`             | `python -m gates` / `hatch run gates` |
| `package.json` scripts      | `pyproject.toml` scripts    |
| ESM imports                 | Standard Python imports     |
| `@types/*` packages         | Type stubs (`*-stubs`)      |

SCAFFOLD_EOF_CONFIG_TSCONFIG_PYTHON-EQUIV_MD

write_file 'llms.txt' << 'SCAFFOLD_EOF_LLMS_TXT_TEMPLATE'
# [Project Name]

> [One-line description]

[Brief paragraph: what it does, how to interact with it.]

## Key Features

- [Feature 1]: [description]
- [Feature 2]: [description]

## Documentation

- [/README.md](README.md) — Project overview
- [/AGENTS.md](AGENTS.md) — Agent integration guide

## Quick Facts

- [Key fact 1]
- [Key fact 2]

SCAFFOLD_EOF_LLMS_TXT_TEMPLATE

write_executable 'scripts/doc-sync-check.sh' << 'SCAFFOLD_EOF_SCRIPTS_DOC-SYNC-CHECK_SH'
#!/bin/bash
# ============================================================================
# Documentation Sync Check — Detects Documentation Drift
# ============================================================================
# Compares "source of truth" code patterns against documentation files to
# find items that exist in code but aren't documented.
#
# How it works:
# 1. Define source patterns (e.g., exported functions, route registrations)
# 2. Define doc files that should reference those items
# 3. Script extracts items from source, checks doc files for references
# 4. Reports any missing references
#
# Usage:
#   ./scripts/doc-sync-check.sh              # Check for drift
#   ./scripts/doc-sync-check.sh --fix        # Report what to add (no auto-fix)
#
# Configuration:
#   Edit the SYNC_RULES array below to match your project structure.
# ============================================================================
set -euo pipefail

FIX_MODE=false
if [ "${1:-}" = "--fix" ]; then
  FIX_MODE=true
fi

# Colors
RED='\033[0;31m'
YELLOW='\033[1;33m'
GREEN='\033[0;32m'
BLUE='\033[0;34m'
NC='\033[0m'

ERRORS=0

# ============================================================================
# CUSTOMIZE: Define your sync rules below.
#
# Each rule has:
#   SOURCE_GLOB  — files to extract items from
#   PATTERN      — regex to extract item names (first capture group)
#   DOC_FILES    — files that must reference each extracted item
#   DESCRIPTION  — human-readable description for error messages
# ============================================================================

check_sync() {
  local description="$1"
  local source_glob="$2"
  local pattern="$3"
  shift 3
  local doc_files=("$@")

  echo -e "${BLUE}Checking: $description${NC}"

  # Extract items from source files
  local items=()
  for source_file in $source_glob; do
    [ -f "$source_file" ] || continue
    while IFS= read -r item; do
      [ -n "$item" ] && items+=("$item")
    done < <(grep -oE "$pattern" "$source_file" 2>/dev/null | sed -E "s/$pattern/\1/" || true)
  done

  if [ ${#items[@]} -eq 0 ]; then
    echo "  No items found in source files. Skipping."
    echo ""
    return
  fi

  echo "  Found ${#items[@]} items in source."

  # Check each doc file for references
  for doc_file in "${doc_files[@]}"; do
    if [ ! -f "$doc_file" ]; then
      echo -e "  ${YELLOW}⚠ Doc file not found: $doc_file${NC}"
      ERRORS=$((ERRORS + 1))
      continue
    fi

    local missing=()
    for item in "${items[@]}"; do
      if ! grep -q "$item" "$doc_file" 2>/dev/null; then
        missing+=("$item")
      fi
    done

    if [ ${#missing[@]} -eq 0 ]; then
      echo -e "  ${GREEN}✓ $doc_file — all items referenced${NC}"
    else
      echo -e "  ${RED}✗ $doc_file — missing ${#missing[@]} reference(s):${NC}"
      for m in "${missing[@]}"; do
        echo "    - $m"
        ERRORS=$((ERRORS + 1))
      done

      if [ "$FIX_MODE" = true ]; then
        echo ""
        echo -e "  ${BLUE}To fix, add these to $doc_file:${NC}"
        for m in "${missing[@]}"; do
          echo "    - \`$m\` — [add description]"
        done
      fi
    fi
  done

  echo ""
}

# ============================================================================
# CUSTOMIZE: Add your sync rules here.
#
# Example rules (uncomment and adapt):
# ============================================================================

# Example 1: Exported functions must be documented in API docs
# check_sync \
#   "Exported functions → API.md" \
#   "src/lib/*.ts" \
#   "export (function|const) ([a-zA-Z]+)" \
#   "docs/API.md"

# Example 2: Route registrations must be in AGENTS.md
# check_sync \
#   "Route registrations → AGENTS.md" \
#   "src/routes/*.ts" \
#   "router\.(get|post|put|delete)\(['\"]([^'\"]+)" \
#   "AGENTS.md"

# Example 3: Environment variables must be in .env.example
# check_sync \
#   "Env vars → .env.example" \
#   "src/**/*.ts" \
#   "process\.env\.([A-Z_]+)" \
#   ".env.example" "CLAUDE.md"

# ============================================================================
# Placeholder: Remove this block once you've added your own rules above
# ============================================================================
echo -e "${YELLOW}No sync rules configured yet.${NC}"
echo ""
echo "Edit this script to add your project's sync rules."
echo "See the CUSTOMIZE section for examples."
echo ""
exit 0

# ============================================================================
# Summary
# ============================================================================
echo "══════════════════════════════════════"
if [ "$ERRORS" -eq 0 ]; then
  echo -e "${GREEN}✓ All documentation is in sync.${NC}"
  exit 0
fi

echo -e "${RED}Found $ERRORS documentation drift issue(s).${NC}"
if [ "$FIX_MODE" = false ]; then
  echo "Run with --fix for details on what to add."
fi
exit 1

SCAFFOLD_EOF_SCRIPTS_DOC-SYNC-CHECK_SH

write_executable 'scripts/security-check.sh' << 'SCAFFOLD_EOF_SCRIPTS_SECURITY-CHECK_SH'
#!/bin/bash
# ============================================================================
# Security Check — Pre-commit Security Scanner
# ============================================================================
# Scans staged files for common security issues. Non-blocking by default
# (exits 0 with warnings). Use --strict to fail on any finding.
#
# What it catches:
# - Path traversal: user input flowing into filesystem operations
# - Console.log in non-test files (if stdout is reserved for data)
# - Hardcoded secrets: API keys, tokens, passwords in source
# - eval() usage: code injection risk
# - SQL string interpolation: SQL injection risk
#
# Usage:
#   ./scripts/security-check.sh              # Warn only (exit 0)
#   ./scripts/security-check.sh --strict     # Fail on findings (exit 1)
# ============================================================================
set -euo pipefail

STRICT=false
if [ "${1:-}" = "--strict" ]; then
  STRICT=true
fi

WARNINGS=0
SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
PROJECT_DIR="$(dirname "$SCRIPT_DIR")"

# Colors
RED='\033[0;31m'
YELLOW='\033[1;33m'
GREEN='\033[0;32m'
NC='\033[0m'

warn() {
  echo -e "${YELLOW}⚠ WARNING:${NC} $1"
  echo "  File: $2"
  echo "  Line: $3"
  echo ""
  WARNINGS=$((WARNINGS + 1))
}

# Get files to check (staged files, or all source files if not in a git context)
if git rev-parse --is-inside-work-tree >/dev/null 2>&1; then
  FILES=$(git diff --cached --name-only --diff-filter=ACM -- '*.ts' '*.tsx' '*.js' '*.jsx' '*.py' 2>/dev/null || true)
  if [ -z "$FILES" ]; then
    # No staged files — check all source files
    FILES=$(find "$PROJECT_DIR/src" -type f \( -name "*.ts" -o -name "*.tsx" -o -name "*.js" -o -name "*.jsx" -o -name "*.py" \) 2>/dev/null || true)
  fi
else
  FILES=$(find "$PROJECT_DIR/src" -type f \( -name "*.ts" -o -name "*.tsx" -o -name "*.js" -o -name "*.jsx" -o -name "*.py" \) 2>/dev/null || true)
fi

if [ -z "$FILES" ]; then
  echo -e "${GREEN}✓ No files to check.${NC}"
  exit 0
fi

echo "Security scan: checking $(echo "$FILES" | wc -l | tr -d ' ') files..."
echo ""

for file in $FILES; do
  [ -f "$file" ] || continue
  LINE_NUM=0

  while IFS= read -r line; do
    LINE_NUM=$((LINE_NUM + 1))

    # Skip test files for console.log check
    IS_TEST=false
    case "$file" in
      *.test.* | *.spec.* | *__tests__* | *test_*) IS_TEST=true ;;
    esac

    # --- Path Traversal ---
    # User input flowing into fs.readFile, fs.writeFile, path.join without sanitization
    if echo "$line" | grep -qE "(readFile|writeFile|readdir|mkdir|unlink|path\.join|path\.resolve)\s*\(.*\b(req\.|request\.|params\.|query\.|body\.|input)" 2>/dev/null; then
      warn "Possible path traversal — user input in filesystem operation" "$file" "$LINE_NUM"
    fi

    # --- Console.log in non-test files ---
    if [ "$IS_TEST" = false ] && echo "$line" | grep -qE "console\.(log|debug)\(" 2>/dev/null; then
      # Skip comments
      TRIMMED=$(echo "$line" | sed 's/^[[:space:]]*//')
      case "$TRIMMED" in
        //*|#*|\**) ;; # skip comments
        *) warn "console.log() in non-test file (use structured logger)" "$file" "$LINE_NUM" ;;
      esac
    fi

    # --- Hardcoded Secrets ---
    # API keys, tokens, passwords assigned as string literals
    if echo "$line" | grep -qiE "(api_key|apikey|secret|password|token|private_key)\s*[:=]\s*['\"][A-Za-z0-9+/=_-]{8,}" 2>/dev/null; then
      # Skip .env.example and template files
      case "$file" in
        *.example | *.template) ;;
        *) warn "Possible hardcoded secret" "$file" "$LINE_NUM" ;;
      esac
    fi

    # --- eval() Usage ---
    if echo "$line" | grep -qE "\beval\s*\(" 2>/dev/null; then
      TRIMMED=$(echo "$line" | sed 's/^[[:space:]]*//')
      case "$TRIMMED" in
        //*|#*|\**) ;; # skip comments
        *) warn "eval() usage — code injection risk" "$file" "$LINE_NUM" ;;
      esac
    fi

    # --- SQL String Interpolation ---
    # Catches template literals and f-strings used in SQL-like contexts
    if echo "$line" | grep -qE "(SELECT|INSERT|UPDATE|DELETE|DROP|ALTER).*(\\\$\{|\" *\+|f['\"])" 2>/dev/null; then
      warn "SQL string interpolation — use parameterized queries" "$file" "$LINE_NUM"
    fi

  done < "$file"
done

# Summary
echo "──────────────────────────────────────"
if [ "$WARNINGS" -eq 0 ]; then
  echo -e "${GREEN}✓ No security issues found.${NC}"
  exit 0
fi

echo -e "${YELLOW}Found $WARNINGS warning(s).${NC}"

if [ "$STRICT" = true ]; then
  echo -e "${RED}Strict mode: failing due to warnings.${NC}"
  exit 1
fi

echo "Run with --strict to treat warnings as errors."
exit 0

SCAFFOLD_EOF_SCRIPTS_SECURITY-CHECK_SH

write_file 'tests/smoke.test.tsx' << 'SCAFFOLD_EOF_TESTS_SMOKE_TEST_TSX'
import { render } from '@testing-library/react';
import { describe, it, expect } from 'vitest';

// CUSTOMIZE: Import your critical components
// import { Button } from '@/components/Button';

describe('Component smoke tests', () => {
  // CUSTOMIZE: Add a smoke test for each critical component
  // it('renders Button without crashing', () => {
  //   const { container } = render(<Button>Click</Button>);
  //   expect(container).toBeTruthy();
  // });

  it('placeholder — add your component smoke tests here', () => {
    expect(true).toBe(true);
  });
});

SCAFFOLD_EOF_TESTS_SMOKE_TEST_TSX

write_file 'tests/test_architecture.py' << 'SCAFFOLD_EOF_TESTS_TEST_ARCHITECTURE_PY'
"""
Architecture Guardrail Test — Python
============================================================================
Enforces architectural rules using AST parsing. Catches violations at test
time before they make it into the codebase.

WHY THIS EXISTS:
LLM agents don't respect implicit architecture rules. They'll import the
database client directly, skip pagination on list queries, and add new
endpoints without registering them. These tests make the rules explicit.

WHAT IT ENFORCES:
1. Single database access point — only db/client.py may import the DB driver
2. Pagination on multi-row queries — every .all() or .filter() must have a limit
3. Registration consistency — every endpoint file must be registered in routes.py

HOW TO CUSTOMIZE:
- Update the constants at the top of each test class
- Add/remove tests based on your project's architecture
- Use the VERIFIED_SINGLE_ROW_LOOKUPS pattern for safe allowlisting

PATTERN: Rule-Bug-Prevention
Rule:    All DB queries must go through the repository layer
Bug:     Direct DB access bypasses caching, logging, and connection management
Prevent: AST scanning catches raw DB imports outside the approved module
============================================================================
"""

import ast
import os
import re
import unittest
from pathlib import Path
from typing import NamedTuple


class Violation(NamedTuple):
    file: str
    line: int
    message: str


# ============================================================================
# CUSTOMIZE: Update these paths to match your project structure
# ============================================================================

# Root directory to scan
SRC_DIR = "src"

# The single file allowed to import the database driver
DB_WRAPPER_MODULE = "src/db/client.py"

# Database packages that should only be imported in the wrapper
FORBIDDEN_DB_IMPORTS = [
    "psycopg2",
    "asyncpg",
    "sqlalchemy.engine",
    "sqlalchemy.create_engine",
]

# Files known to perform single-row lookups (no pagination needed).
# Add entries here after manually verifying the query is safe.
# Format: "path/to/file.py:function_name"
VERIFIED_SINGLE_ROW_LOOKUPS: set[str] = {
    # "src/users/repository.py:get_by_id",
    # "src/auth/repository.py:get_session",
}

# Route registration file
ROUTES_FILE = "src/routes.py"

# Pattern that registers an endpoint (customize the regex)
ROUTE_REGISTRATION_PATTERN = re.compile(
    r"router\.(include_router|add_api_route|get|post|put|delete)\("
)

# Directory containing endpoint files
ENDPOINTS_DIR = "src/endpoints"


def get_python_files(directory: str) -> list[str]:
    """Recursively get all Python files in a directory."""
    files = []
    root = Path(directory)

    if not root.exists():
        return files

    for path in root.rglob("*.py"):
        # Skip test files, __pycache__, and virtual envs
        parts = path.parts
        if any(
            p in ("__pycache__", ".venv", "venv", "node_modules", ".git")
            for p in parts
        ):
            continue
        if path.name.startswith("test_") or path.name.endswith("_test.py"):
            continue
        files.append(str(path))

    return files


def parse_file(filepath: str) -> ast.Module | None:
    """Parse a Python file into an AST, returning None on failure."""
    try:
        with open(filepath) as f:
            return ast.parse(f.read(), filename=filepath)
    except (SyntaxError, UnicodeDecodeError):
        return None


class TestDatabaseAccessBoundary(unittest.TestCase):
    """
    Enforces single database access point.

    Rule:    Only db/client.py may import database driver packages.
    Bug:     Direct DB imports bypass connection pooling and error handling.
    Prevent: This test scans all Python files for forbidden DB imports.
    """

    def test_no_direct_db_imports(self) -> None:
        violations: list[Violation] = []
        files = get_python_files(SRC_DIR)
        wrapper = os.path.normpath(DB_WRAPPER_MODULE)

        for filepath in files:
            normalized = os.path.normpath(filepath)
            if normalized == wrapper:
                continue

            tree = parse_file(filepath)
            if tree is None:
                continue

            for node in ast.walk(tree):
                # Check 'import X' statements
                if isinstance(node, ast.Import):
                    for alias in node.names:
                        for forbidden in FORBIDDEN_DB_IMPORTS:
                            if alias.name == forbidden or alias.name.startswith(
                                forbidden + "."
                            ):
                                violations.append(
                                    Violation(
                                        file=filepath,
                                        line=node.lineno,
                                        message=(
                                            f"Direct import of '{alias.name}' — "
                                            f"use {DB_WRAPPER_MODULE} instead"
                                        ),
                                    )
                                )

                # Check 'from X import Y' statements
                if isinstance(node, ast.ImportFrom) and node.module:
                    for forbidden in FORBIDDEN_DB_IMPORTS:
                        if node.module == forbidden or node.module.startswith(
                            forbidden + "."
                        ):
                            violations.append(
                                Violation(
                                    file=filepath,
                                    line=node.lineno,
                                    message=(
                                        f"Direct import from '{node.module}' — "
                                        f"use {DB_WRAPPER_MODULE} instead"
                                    ),
                                )
                            )

        if violations:
            msg = f"\nFound {len(violations)} forbidden DB import(s):\n\n"
            for v in violations:
                msg += f"  {v.file}:{v.line} — {v.message}\n"
            msg += f"\nOnly {DB_WRAPPER_MODULE} may import these packages."
            self.fail(msg)


class TestPaginationEnforcement(unittest.TestCase):
    """
    Enforces pagination on multi-row queries.

    Rule:    Every query that can return multiple rows must include a limit.
    Bug:     Unbounded queries cause memory issues and slow responses at scale.
    Prevent: AST scanning checks for .all()/.filter() without .limit().
    """

    def test_multi_row_queries_have_pagination(self) -> None:
        violations: list[Violation] = []
        files = get_python_files(SRC_DIR)

        # Patterns that indicate multi-row queries
        multi_row_patterns = re.compile(
            r"\.(all|filter|filter_by|select|query)\s*\("
        )

        # Patterns that indicate pagination is present
        pagination_patterns = re.compile(
            r"\.(limit|paginate|slice|first|\[:|\[0:)\s*\("
        )

        for filepath in files:
            with open(filepath) as f:
                lines = f.readlines()

            for i, line in enumerate(lines, 1):
                if multi_row_patterns.search(line):
                    # Check if this is a verified single-row lookup
                    func_name = _get_enclosing_function(filepath, i)
                    lookup_key = f"{filepath}:{func_name}" if func_name else ""

                    if lookup_key in VERIFIED_SINGLE_ROW_LOOKUPS:
                        continue

                    # Check surrounding lines (within 5 lines) for pagination
                    context_start = max(0, i - 3)
                    context_end = min(len(lines), i + 5)
                    context = "".join(lines[context_start:context_end])

                    if not pagination_patterns.search(context):
                        violations.append(
                            Violation(
                                file=filepath,
                                line=i,
                                message=(
                                    f"Multi-row query without pagination: "
                                    f"{line.strip()}"
                                ),
                            )
                        )

        if violations:
            msg = f"\nFound {len(violations)} query(ies) without pagination:\n\n"
            for v in violations:
                msg += f"  {v.file}:{v.line} — {v.message}\n"
            msg += (
                "\nFix: Add .limit() or add to VERIFIED_SINGLE_ROW_LOOKUPS "
                "if this is intentionally unbounded."
            )
            self.fail(msg)


class TestRegistrationConsistency(unittest.TestCase):
    """
    Enforces that every endpoint module is registered in routes.

    Rule:    Every file in endpoints/ must be imported in routes.py.
    Bug:     New endpoints that aren't registered are silently unreachable.
    Prevent: This test compares endpoint files to route registrations.
    """

    def test_all_endpoints_registered(self) -> None:
        if not os.path.exists(ENDPOINTS_DIR):
            self.skipTest(f"No endpoints directory at {ENDPOINTS_DIR}")

        if not os.path.exists(ROUTES_FILE):
            self.skipTest(f"No routes file at {ROUTES_FILE}")

        # Get endpoint module names
        endpoint_modules = set()
        for filepath in get_python_files(ENDPOINTS_DIR):
            module_name = Path(filepath).stem
            if module_name != "__init__":
                endpoint_modules.add(module_name)

        if not endpoint_modules:
            return

        # Read routes file and find registered modules
        with open(ROUTES_FILE) as f:
            routes_content = f.read()

        unregistered = []
        for module in sorted(endpoint_modules):
            if module not in routes_content:
                unregistered.append(module)

        if unregistered:
            msg = f"\nFound {len(unregistered)} unregistered endpoint(s):\n\n"
            for module in unregistered:
                msg += f"  - {module} (in {ENDPOINTS_DIR}/{module}.py)\n"
            msg += f"\nRegister them in {ROUTES_FILE}."
            self.fail(msg)


def _get_enclosing_function(filepath: str, line_number: int) -> str | None:
    """Get the name of the function containing a given line number."""
    tree = parse_file(filepath)
    if tree is None:
        return None

    for node in ast.walk(tree):
        if isinstance(node, (ast.FunctionDef, ast.AsyncFunctionDef)):
            if hasattr(node, "end_lineno") and node.end_lineno:
                if node.lineno <= line_number <= node.end_lineno:
                    return node.name

    return None


if __name__ == "__main__":
    unittest.main()

SCAFFOLD_EOF_TESTS_TEST_ARCHITECTURE_PY

write_file 'tests/test_architecture.ts' << 'SCAFFOLD_EOF_TESTS_TEST_ARCHITECTURE_TS'
/**
 * Architecture Guardrail Test — Import Boundary Enforcement
 * ============================================================================
 * This test enforces architectural boundaries by preventing direct imports of
 * protected modules. Only a designated wrapper module is allowed to import them.
 *
 * WHY THIS EXISTS:
 * LLM agents don't know your architecture. They'll import the database client
 * directly from any file, bypassing your connection pooling, error handling,
 * and logging wrappers. This test catches that at CI time.
 *
 * HOW TO CUSTOMIZE:
 * 1. Change FORBIDDEN_IMPORTS to match YOUR protected modules
 * 2. Change ALLOWED_IMPORTERS to match YOUR wrapper module(s)
 * 3. Change SCAN_DIR to match YOUR source directory
 *
 * PATTERN: Rule-Bug-Prevention
 * Rule:    Only db/client.ts may import from 'pg' / 'prisma' / etc.
 * Bug:     Direct DB imports bypass connection pooling, skip error handling
 * Prevent: This test fails CI if any other file imports the DB client directly
 * ============================================================================
 */

import { describe, it, expect } from 'vitest';
import * as fs from 'node:fs';
import * as path from 'node:path';

// ============================================================================
// CUSTOMIZE: Configure these values for your project
// ============================================================================

/** Modules that should only be imported through a wrapper */
const FORBIDDEN_IMPORTS: string[] = [
  // CUSTOMIZE: Add your protected module patterns here
  // Examples:
  // 'pg',
  // '@prisma/client',
  // 'redis',
  // 'aws-sdk',
  'pg',
  '@prisma/client',
];

/** Files that ARE allowed to import the forbidden modules (the wrappers) */
const ALLOWED_IMPORTERS: string[] = [
  // CUSTOMIZE: Your wrapper module(s) that encapsulate the protected imports
  // Examples:
  // 'src/lib/db.ts',
  // 'src/lib/redis.ts',
  'src/lib/db.ts',
  'src/db/client.ts',
];

/** Directory to scan for violations */
const SCAN_DIR = 'src';

// ============================================================================
// Test implementation — you shouldn't need to modify below this line
// ============================================================================

/**
 * Recursively get all TypeScript files in a directory
 */
function getTypeScriptFiles(dir: string): string[] {
  const files: string[] = [];

  if (!fs.existsSync(dir)) {
    return files;
  }

  const entries = fs.readdirSync(dir, { withFileTypes: true });

  for (const entry of entries) {
    const fullPath = path.join(dir, entry.name);

    if (entry.isDirectory()) {
      // Skip node_modules and build output
      if (entry.name === 'node_modules' || entry.name === 'dist' || entry.name === 'build') {
        continue;
      }
      files.push(...getTypeScriptFiles(fullPath));
    } else if (entry.name.endsWith('.ts') || entry.name.endsWith('.tsx')) {
      // Skip test files — they may import anything for testing purposes
      if (entry.name.includes('.test.') || entry.name.includes('.spec.')) {
        continue;
      }
      files.push(fullPath);
    }
  }

  return files;
}

/**
 * Check if a file contains forbidden imports
 */
function findForbiddenImports(
  filePath: string,
  forbiddenModules: string[],
): { module: string; line: number; text: string }[] {
  const content = fs.readFileSync(filePath, 'utf-8');
  const lines = content.split('\n');
  const violations: { module: string; line: number; text: string }[] = [];

  for (let i = 0; i < lines.length; i++) {
    const line = lines[i]!;

    for (const mod of forbiddenModules) {
      // Match: import ... from 'module'
      // Match: import 'module'
      // Match: require('module')
      const patterns = [
        new RegExp(`from\\s+['"]${escapeRegex(mod)}['"]`),
        new RegExp(`import\\s+['"]${escapeRegex(mod)}['"]`),
        new RegExp(`require\\s*\\(\\s*['"]${escapeRegex(mod)}['"]`),
      ];

      for (const pattern of patterns) {
        if (pattern.test(line)) {
          violations.push({
            module: mod,
            line: i + 1,
            text: line.trim(),
          });
        }
      }
    }
  }

  return violations;
}

function escapeRegex(str: string): string {
  return str.replace(/[.*+?^${}()|[\]\\]/g, '\\$&');
}

// ============================================================================
// Tests
// ============================================================================

describe('Architecture: Import boundaries', () => {
  it('should not import protected modules outside of designated wrappers', () => {
    const files = getTypeScriptFiles(SCAN_DIR);
    const violations: string[] = [];

    // Normalize allowed importers for comparison
    const normalizedAllowed = ALLOWED_IMPORTERS.map((f) =>
      path.normalize(f),
    );

    for (const file of files) {
      const normalizedFile = path.normalize(file);

      // Skip if this file is an allowed importer
      if (normalizedAllowed.some((allowed) => normalizedFile.endsWith(allowed))) {
        continue;
      }

      const found = findForbiddenImports(file, FORBIDDEN_IMPORTS);

      for (const violation of found) {
        violations.push(
          `${file}:${violation.line} imports '${violation.module}' directly.\n` +
            `  Found: ${violation.text}\n` +
            `  Only these files may import '${violation.module}': ${ALLOWED_IMPORTERS.join(', ')}`,
        );
      }
    }

    if (violations.length > 0) {
      const message =
        `Found ${violations.length} forbidden import(s):\n\n` +
        violations.join('\n\n') +
        '\n\n' +
        'Fix: Import from the wrapper module instead of the direct dependency.\n' +
        `Allowed wrappers: ${ALLOWED_IMPORTERS.join(', ')}`;

      expect.fail(message);
    }
  });
});

SCAFFOLD_EOF_TESTS_TEST_ARCHITECTURE_TS

write_file 'tests/test_workspace_boundaries.ts' << 'SCAFFOLD_EOF_TESTS_TEST_WORKSPACE_BOUNDARIES_TS'
/**
 * Workspace Boundary Guardrail Test — Monorepo Import Enforcement
 * ============================================================================
 * CUSTOMIZE: Update WORKSPACE_RULES to match your monorepo package structure.
 * Skip this file entirely for single-package repos.
 *
 * WHY THIS EXISTS:
 * In monorepos, LLM agents create accidental circular dependencies by importing
 * between packages that shouldn't know about each other. This test enforces
 * explicit dependency boundaries between workspace packages.
 *
 * HOW IT WORKS:
 * 1. Define allowed/denied import relationships in WORKSPACE_RULES
 * 2. Test scans each package's source files for cross-package imports
 * 3. Flags any import that violates the allow/deny rules
 *
 * EXAMPLE VIOLATION MESSAGE:
 * "packages/ui/src/Button.tsx imports from packages/api — this is not allowed.
 *  packages/ui may only import from: packages/shared"
 * ============================================================================
 */

import { describe, it, expect } from 'vitest';
import * as fs from 'node:fs';
import * as path from 'node:path';

// ============================================================================
// CUSTOMIZE: Define your workspace dependency rules
// ============================================================================

/**
 * Import rules for each package in the monorepo.
 *
 * - allow: packages this package MAY import from
 * - deny: packages this package MUST NOT import from
 *
 * If a package is in neither allow nor deny, it's treated as denied by default
 * (explicit-allow model). To use explicit-deny instead, flip the logic in
 * checkViolation().
 */
const WORKSPACE_RULES: Record<string, { allow: string[]; deny: string[] }> = {
  // CUSTOMIZE: Replace with your actual package structure
  'packages/ui': {
    allow: ['packages/shared'],
    deny: ['packages/api', 'packages/db'],
  },
  'packages/api': {
    allow: ['packages/shared', 'packages/db'],
    deny: ['packages/ui'],
  },
  'packages/shared': {
    allow: [],
    deny: ['packages/ui', 'packages/api', 'packages/db'],
  },
};

/** Root directory of the monorepo (relative to project root) */
const MONOREPO_ROOT = '.';

// ============================================================================
// Implementation — you shouldn't need to modify below this line
// ============================================================================

interface Violation {
  sourceFile: string;
  sourcePackage: string;
  importedPackage: string;
  line: number;
  text: string;
}

/**
 * Get all TypeScript/JavaScript files in a directory recursively
 */
function getSourceFiles(dir: string): string[] {
  const files: string[] = [];

  if (!fs.existsSync(dir)) {
    return files;
  }

  const entries = fs.readdirSync(dir, { withFileTypes: true });

  for (const entry of entries) {
    const fullPath = path.join(dir, entry.name);

    if (entry.isDirectory()) {
      if (['node_modules', 'dist', 'build', '.next', 'coverage'].includes(entry.name)) {
        continue;
      }
      files.push(...getSourceFiles(fullPath));
    } else if (/\.(ts|tsx|js|jsx)$/.test(entry.name)) {
      files.push(fullPath);
    }
  }

  return files;
}

/**
 * Extract cross-package import references from a file
 */
function findCrossPackageImports(
  filePath: string,
  allPackages: string[],
): { importedPackage: string; line: number; text: string }[] {
  const content = fs.readFileSync(filePath, 'utf-8');
  const lines = content.split('\n');
  const results: { importedPackage: string; line: number; text: string }[] = [];

  for (let i = 0; i < lines.length; i++) {
    const line = lines[i]!;

    // Match import/require statements
    const importPatterns = [
      /from\s+['"]([^'"]+)['"]/,
      /require\s*\(\s*['"]([^'"]+)['"]/,
      /import\s*\(\s*['"]([^'"]+)['"]/,
    ];

    for (const pattern of importPatterns) {
      const match = line.match(pattern);
      if (!match) continue;

      const importPath = match[1]!;

      // Check if this import references another workspace package
      for (const pkg of allPackages) {
        // Match imports like '@scope/package-name' or relative paths to other packages
        const pkgName = path.basename(pkg);
        const scopedName = `@${path.basename(path.dirname(pkg))}/${pkgName}`;

        if (
          importPath === pkgName ||
          importPath.startsWith(pkgName + '/') ||
          importPath === scopedName ||
          importPath.startsWith(scopedName + '/') ||
          importPath.includes(`../${pkg}`) ||
          importPath.includes(`../../${pkg}`)
        ) {
          results.push({
            importedPackage: pkg,
            line: i + 1,
            text: line.trim(),
          });
        }
      }
    }
  }

  return results;
}

/**
 * Determine which package a file belongs to
 */
function getPackageForFile(filePath: string, packages: string[]): string | null {
  const normalized = path.normalize(filePath);
  for (const pkg of packages) {
    if (normalized.startsWith(path.normalize(pkg) + path.sep)) {
      return pkg;
    }
  }
  return null;
}

// ============================================================================
// Tests
// ============================================================================

describe('Workspace boundaries: cross-package imports', () => {
  const packages = Object.keys(WORKSPACE_RULES);

  it('should not import from denied packages', () => {
    const violations: Violation[] = [];

    for (const sourcePackage of packages) {
      const srcDir = path.join(MONOREPO_ROOT, sourcePackage, 'src');
      const files = getSourceFiles(srcDir);
      const rules = WORKSPACE_RULES[sourcePackage]!;

      // All other packages that aren't explicitly allowed
      const otherPackages = packages.filter((p) => p !== sourcePackage);

      for (const file of files) {
        const imports = findCrossPackageImports(file, otherPackages);

        for (const imp of imports) {
          // Check if this import is allowed
          const isAllowed = rules.allow.includes(imp.importedPackage);
          const isDenied = rules.deny.includes(imp.importedPackage);

          // Violation if explicitly denied, or if not explicitly allowed
          if (isDenied || !isAllowed) {
            violations.push({
              sourceFile: file,
              sourcePackage,
              importedPackage: imp.importedPackage,
              line: imp.line,
              text: imp.text,
            });
          }
        }
      }
    }

    if (violations.length > 0) {
      const grouped = new Map<string, Violation[]>();
      for (const v of violations) {
        const key = `${v.sourcePackage} → ${v.importedPackage}`;
        if (!grouped.has(key)) grouped.set(key, []);
        grouped.get(key)!.push(v);
      }

      let message = `Found ${violations.length} workspace boundary violation(s):\n\n`;

      for (const [key, group] of grouped) {
        const sourcePackage = group[0]!.sourcePackage;
        const importedPackage = group[0]!.importedPackage;
        const rules = WORKSPACE_RULES[sourcePackage]!;
        const allowedStr =
          rules.allow.length > 0 ? rules.allow.join(', ') : '(none — this is a leaf package)';

        message += `${key}:\n`;
        for (const v of group) {
          message += `  ${v.sourceFile}:${v.line} — ${v.text}\n`;
        }
        message += `  ${sourcePackage} may only import from: ${allowedStr}\n\n`;
      }

      message += 'Fix: Move shared code to an allowed package, or update WORKSPACE_RULES.';

      expect.fail(message);
    }
  });
});

SCAFFOLD_EOF_TESTS_TEST_WORKSPACE_BOUNDARIES_TS


# Set up the gates script in package.json
setup_gates() {
  if [ ! -f "$TARGET_DIR/package.json" ]; then
    echo ""
    echo "Creating package.json with gates script..."
    local run_cmd="npm run"
    if [ "$PKG_MANAGER" = "pnpm" ]; then
      run_cmd="pnpm"
    elif [ "$PKG_MANAGER" = "yarn" ]; then
      run_cmd="yarn"
    fi

    cat > "$TARGET_DIR/package.json" << PKGJSON
{
  "name": "my-project",
  "version": "0.0.1",
  "private": true,
  "scripts": {
    "dev": "echo 'Add your dev command'",
    "lint": "biome check .",
    "typecheck": "tsc --noEmit",
    "test": "vitest run",
    "build": "echo 'Add your build command'",
    "gates": "$run_cmd lint && $run_cmd typecheck && $run_cmd test && $run_cmd build"
  }
}
PKGJSON
    echo "  Created: package.json"
  else
    echo ""
    echo "package.json already exists. Add this script manually:"
    echo ""
    echo '  "gates": "npm run lint && npm run typecheck && npm run test && npm run build"'
    echo ""
  fi
}

setup_gates

echo ""
echo "✓ Scaffold initialized in $TARGET_DIR"
echo ""
echo "Next steps:"
echo "  1. Edit CLAUDE.md — replace all [bracketed] values"
echo "  2. Edit .env.example — add your environment variables"
echo "  3. Edit .claude/settings.json — adjust allowed commands"
echo "  4. Run: npm install husky --save-dev && npx husky init"
echo "  5. Run: npm run gates (verify everything passes)"
echo ""
echo "Optional:"
echo "  - Edit NOW.md if project will last > 2 weeks"
echo "  - Edit AGENTS.md if project exposes an API"
echo "  - Delete files you don't need (see docs/DECISION-TREES.md)"
